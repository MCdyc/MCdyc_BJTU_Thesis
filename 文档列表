[1] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners[J]. Advances in neural information processing systems, 2020, 33: 1877-1901.

[2] Achiam J, Adler S, Agarwal S, et al. Gpt-4 technical report[J]. arXiv preprint arXiv:2303.08774, 2023.

[3] Hoffmann J, Borgeaud S, Mensch A, et al. Training compute-optimal large language models[C]\/\/Proceedings of the 36th International Conference on Neural Information Processing Systems. 2022: 30016-30030.

[4] Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models[J]. arXiv preprint arXiv:2302.13971, 2023.

[5] Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.

[6] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[C]\/\/Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 2019: 4171-4186.

[7] Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer[J]. Journal of machine learning research, 2020, 21(140): 1-67.

[8] Lewis P, Perez E, Piktus A, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks[J]. Advances in neural information processing systems, 2020, 33: 9459-9474.

[9] Borgeaud S, Mensch A, Hoffmann J, et al. Improving language models by retrieving from trillions of tokens[C]\/\/International conference on machine learning. PMLR, 2022: 2206-2240.

[10] Gao Y, Xiong Y, Gao X, et al. Retrieval-Augmented Generation for Large Language Models: A Survey[J]. CoRR, 2023.

[11] Khandelwal U, Levy O, Jurafsky D, et al. Generalization through Memorization: Nearest Neighbor Language Models[C]\/\/International Conference on Learning Representations.

[12] Zhang Z, Dai Q, Bo X, et al. A survey on the memory mechanism of large language model-based agents[J]. ACM Transactions on Information Systems, 2025, 43(6): 1-47.

[13] Zhong W, Guo L, Gao Q, et al. Memorybank: Enhancing large language models with long-term memory[C]\/\/Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(17): 19724-19731.

[14] Fathullah Y, Wu C, Lakomkin E, et al. Audiochatllama: Towards general-purpose speech abilities for llms[C]\/\/Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2024: 5522-5532.

[15] Arandas L, Grierson M, Carvalhais M. Antagonising explanation and revealing bias directly through sequencing and multimodal inference[J]. arXiv e-prints, 2023: arXiv: 2309.12345.

[16] Shinn N, Cassano F, Gopinath A, et al. Reflexion: Language agents with verbal reinforcement learning[J]. Advances in Neural Information Processing Systems, 2023, 36: 8634-8652.

[17] HU M, MU Y, YU X, et al. Tree-Planner: Efficient Close-loop Task Planning with Large Language Models[C]\/\/ Kim B, Yue Y, Chaudhuri S, et al. International Conference on Learning Representations. 2024: 48669-48696.

[18] Masson S J, Yan Z, Ho J, et al. State-insensitive wavelengths for light shifts and photon scattering from Zeeman states[J]. Physical Review A, 2024, 109(6): 063105.

[19] Xu W, Mei K, Gao H, et al. A-mem: Agentic memory for llm agents[J]. arXiv preprint arXiv:2502.12110, 2025.

[20] Xiong Z, Lin Y, Xie W, et al. How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior[J]. arXiv e-prints, 2025: arXiv: 2505.16067.

[21] Yang C, Yang X, Wen L, et al. Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks[J]. arXiv preprint arXiv:2510.08002, 2025.

[22] Chatterjee M, Agarwal D. Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context[J]. arXiv preprint arXiv:2508.12630, 2025.

[23] Lu B R, Hu Y, Cheng H, et al. Unsupervised Learning of Hierarchical Conversation Structure[C]\/\/Findings of the Association for Computational Linguistics: EMNLP 2022. 2022: 5657-5670.

[24] Ou L, Lapata M. Context-aware hierarchical merging for long document summarization[J]. arXiv preprint arXiv:2502.00977, 2025.

[25] Park J S, O'Brien J, Cai C J, et al. Generative agents: Interactive simulacra of human behavior[C]\/\/Proceedings of the 36th annual acm symposium on user interface software and technology. 2023: 1-22.

[26] Wang G, Xie Y, Jiang Y, et al. Voyager: An Open-Ended Embodied Agent with Large Language Models[J]. Transactions on Machine Learning Research.

[27] Ake D, Bouzas A O, Larios F. Top Quark Flavor Changing Couplings at a Muon Collider, Adv[J]. High Energy Phys, 2024, 2038180(2311.09488).

[28] Liu X, Yu H, Zhang H, et al. AgentBench: Evaluating LLMs as Agents[C]\/\/The Twelfth International Conference on Learning Representations.

[29] Yan S, Yang X, Huang Z, et al. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning[J]. arXiv preprint arXiv:2508.19828, 2025.

[30] Zhang Z, Dai Q, Li R, et al. Learn to memorize: Optimizing llm-based agents with adaptive memory framework[J]. arXiv preprint arXiv:2508.16629, 2025.

[31] Ai Q, Tang Y, Wang C, et al. MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems[J]. arXiv preprint arXiv:2510.17281, 2025.

